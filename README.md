# Software Engineering Evals for AI Coding Assistants

**ðŸŽ‰ Complete Benchmark Suite - 20/20 Benchmarks (100% Coverage)**

A comprehensive, production-ready benchmark suite for evaluating AI coding assistants across all 20 software engineering evaluation areas.

## Overview

This repository contains automated, verifiable benchmarks that test AI's ability to complete real-world software development tasks. Each benchmark provides:
- Clear task specifications with realistic complexity
- Automated scoring with objective metrics (>90% automation rate)
- Reproducible evaluation results (Â±0% variance)
- Standardized JSON output for analysis
- Comprehensive test suites (~600 total tests)

## Project Stats

- **Total Benchmarks:** 20 (100% coverage)
- **Total Files:** ~250+ files
- **Total Lines of Code:** ~35,000 LOC
- **Total Test Cases:** ~600+ tests
- **Documentation:** ~90+ markdown files
- **Automation Rate:** >90% across all benchmarks
- **Development Time:** ~8 hours total
- **Pass Threshold:** 70/100 (standardized)

## Current Benchmarks

### Tier 1: Core Capabilities (5/5 Complete) âœ…

| Benchmark | Category | Description | Difficulty | Status |
|-----------|----------|-------------|------------|--------|
| [bug-fixing-001](benchmarks/bug-fixing-001/) | Quality | Fix off-by-one error in date calculation | Easy | âœ… Complete |
| [testing-001](benchmarks/testing-001/) | Quality | Write comprehensive tests with mutation testing | Medium | âœ… Complete |
| [greenfield-001](benchmarks/greenfield-001/) | Creation | Build URL shortener REST API from scratch | Medium-Hard | âœ… Complete |
| [refactoring-001](benchmarks/refactoring-001/) | Evolution | Improve code structure while preserving behavior | Medium-Hard | âœ… Complete |
| [code-migration-001](benchmarks/code-migration-001/) | Evolution | Migrate SQLAlchemy 1.4 â†’ 2.0 | Medium-Hard | âœ… Complete |

### Tier 2: Advanced Capabilities (7/7 Complete) âœ…

| Benchmark | Category | Description | Difficulty | Status |
|-----------|----------|-------------|------------|--------|
| [debugging-001](benchmarks/debugging-001/) | Quality | Identify root cause of LRU cache eviction bug | Medium | âœ… Complete |
| [maintenance-001](benchmarks/maintenance-001/) | Evolution | Update dependencies and fix real CVEs | Medium | âœ… Complete |
| [documentation-001](benchmarks/documentation-001/) | Knowledge | Document undocumented HTTP client library | Easy | âœ… Complete |
| [rewriting-001](benchmarks/rewriting-001/) | Evolution | Rewrite recursive tree functions as iterative | Medium | âœ… Complete |
| [code-review-001](benchmarks/code-review-001/) | Quality | Find 11 planted bugs in pull request | Medium | âœ… Complete |
| [api-design-001](benchmarks/api-design-001/) | Creation | Design OpenAPI 3.0 spec for e-commerce | Medium-Hard | âœ… Complete |
| [data-modelling-001](benchmarks/data-modelling-001/) | Creation | Design database schema for blog platform | Medium | âœ… Complete |

### Tier 3: Expert Capabilities (8/8 Complete) âœ…

| Benchmark | Category | Description | Difficulty | Status |
|-----------|----------|-------------|------------|--------|
| [security-001](benchmarks/security-001/) | Quality | Fix 10 OWASP vulnerabilities in Flask app | Hard | âœ… Complete |
| [performance-001](benchmarks/performance-001/) | Quality | Optimize O(nÂ²) code using profiler data | Medium-Hard | âœ… Complete |
| [legacy-comprehension-001](benchmarks/legacy-comprehension-001/) | Knowledge | Answer 20 Q&A about 845-line legacy system | Medium | âœ… Complete |
| [architecture-001](benchmarks/architecture-001/) | Creation | Design real-time collaborative platform | Hard | âœ… Complete |
| [concurrency-001](benchmarks/concurrency-001/) | Quality | Fix race conditions in concurrent code | Medium | âœ… Complete |
| [prototyping-001](benchmarks/prototyping-001/) | Creation | Build file-watching CLI tool POC | Easy | âœ… Complete |
| [infrastructure-001](benchmarks/infrastructure-001/) | Operations | Write Terraform for AWS deployment | Medium-Hard | âœ… Complete |
| [porting-001](benchmarks/porting-001/) | Evolution | Port Python text analyzer to TypeScript | Medium | âœ… Complete |

## Coverage: 100% (20/20 Evaluation Areas)

### âœ… Creation (5/5)
- [x] Greenfield (greenfield-001)
- [x] Prototyping/Spike (prototyping-001)
- [x] Architecture (architecture-001)
- [x] API Design (api-design-001)
- [x] Data Modelling (data-modelling-001)

### âœ… Evolution (5/5)
- [x] Maintenance (maintenance-001)
- [x] Refactoring (refactoring-001)
- [x] Rewriting (rewriting-001)
- [x] Porting (porting-001)
- [x] Code Migration (code-migration-001)

### âœ… Quality (7/7)
- [x] Debugging (debugging-001)
- [x] Bug Fixing (bug-fixing-001)
- [x] Testing (testing-001)
- [x] Code Review (code-review-001)
- [x] Performance Optimisation (performance-001)
- [x] Security (security-001)
- [x] Concurrency (concurrency-001)

### âœ… Knowledge (2/2)
- [x] Documentation (documentation-001)
- [x] Legacy Code Comprehension (legacy-comprehension-001)

### âœ… Operations (1/1)
- [x] Infrastructure (infrastructure-001)

## Quick Start

### Running a Single Benchmark

```bash
# Navigate to a benchmark directory
cd benchmarks/bug-fixing-001

# Read the task specification
cat spec.md

# Read the prompt for AI
cat prompts.txt

# Complete the task (e.g., fix the bug)
# ...

# Run verification
./verification/verify.sh
```

## Key Achievements

### ðŸŽ¯ Complete Coverage
- **100% of Software Development Lifecycle** - Every major area from greenfield development to infrastructure operations
- **20 Production-Ready Benchmarks** - Realistic, challenging tasks that mirror real-world scenarios
- **Comprehensive Evaluation** - Tests creation, evolution, quality, knowledge, and operations

### ðŸ“Š High Quality
- **>90% Automation** - Deterministic scoring minimizes subjectivity
- **600+ Test Cases** - Comprehensive validation across all benchmarks
- **Reproducible Results** - Â±0% variance on same AI/code
- **Clear Success Criteria** - Standardized 70/100 pass threshold

### ðŸš€ Production-Ready
- **Realistic Complexity** - Based on real production code patterns
- **Well-Documented** - 90+ markdown files with specs, prompts, READMEs
- **Validated Benchmarks** - All tested with buggy and correct code
- **Standardized Output** - JSON scoring for automated analysis

### ðŸ”§ Developer-Friendly
- **Template System** - Easy to create new benchmarks
- **Automated Verification** - One-command scoring with `./verification/verify.sh`
- **Multiple Languages** - Python, JavaScript/TypeScript, Terraform
- **Extensible Framework** - Clean architecture for future additions

### Verification Output

Each benchmark outputs JSON with scoring details:

```json
{
  "benchmark": "bug-fixing-001",
  "timestamp": "2026-02-01T00:00:00Z",
  "components": {
    "bug_fixed": {"score": 100, "weight": 0.6, "details": "..."},
    "no_regressions": {"score": 100, "weight": 0.3, "details": "..."},
    "code_quality": {"score": 100, "weight": 0.1, "details": "..."}
  },
  "base_score": 100,
  "penalties": {
    "time_penalty": 0,
    "iteration_penalty": 0,
    "error_penalty": 0
  },
  "final_score": 100,
  "passed": true
}
```

## Benchmark Highlights

### Security (security-001)
- **10 OWASP Top 10 vulnerabilities** in realistic e-commerce Flask app
- SQL injection, XSS, command injection, insecure deserialization, hardcoded secrets
- SAST tool integration (Bandit) for automated scanning
- 7 critical + 3 high severity issues

### Performance (performance-001)
- **Intentionally slow O(nÂ²) implementation** (15 second baseline)
- Real cProfile output showing bottlenecks
- Reference solution achieves **3,750x speedup**
- Target: 10x improvement minimum

### Architecture (architecture-001)
- **Design real-time collaborative document editing platform**
- Complex requirements: 100k concurrent users, <100ms latency
- LLM-as-judge evaluation of ADRs, diagrams, trade-offs
- Example submission: 1,897 lines of architecture documentation

### Concurrency (concurrency-001)
- **3 realistic race condition patterns** (cache, counter, worker pool)
- Tests must pass **100 consecutive times** to verify thread safety
- Stress tests with thousands of concurrent operations
- Validates proper synchronization primitives

### Legacy Comprehension (legacy-comprehension-001)
- **845 lines of undocumented legacy code** across 6 files
- 20 questions on architecture, dependencies, data flow, impact analysis
- Fuzzy matching for answer evaluation
- Weighted question difficulty

### Infrastructure (infrastructure-001)
- **Complete AWS infrastructure** (VPC, ECS Fargate, RDS, ALB, S3)
- Security best practices (Secrets Manager, encryption, IAM)
- Idempotency validation
- terraform plan validation (no actual deployment)

## Benchmark Structure

Each benchmark follows a standard structure:

```
benchmark-name/
â”œâ”€â”€ README.md              # Benchmark-specific documentation
â”œâ”€â”€ spec.md               # Task specification for AI
â”œâ”€â”€ prompts.txt           # Standard prompt format
â”œâ”€â”€ starter-code/         # Initial codebase (if applicable)
â”‚   â””â”€â”€ ...
â”œâ”€â”€ data/                 # Sample data (if applicable)
â”‚   â””â”€â”€ ...
â”œâ”€â”€ verification/         # Automated verification
â”‚   â”œâ”€â”€ verify.sh        # Main verification script
â”‚   â””â”€â”€ tests/           # Test suite
â””â”€â”€ reference-solution/   # Reference implementation (hidden from AI)
    â””â”€â”€ ...
```

## Evaluation Criteria

Benchmarks are scored on:
- **Automation Rate**: >70% of scoring is deterministic
- **Reproducibility**: Same AI produces same score Â±5%
- **Discrimination**: Different approaches produce different scores
- **Face Validity**: Scores align with human judgment

## Research & Usage

### For AI Development Teams
- Benchmark new model versions
- Track improvement over time
- Identify capability gaps
- Guide training priorities

### For Researchers
- Compare different AI architectures
- Study reasoning patterns
- Publish evaluation results
- Advance AI coding research

### For End Users
- Choose the right AI coding assistant
- Understand AI strengths/weaknesses
- Set realistic expectations
- Make informed decisions

### Running Evaluations

```bash
# List all benchmarks
python evaluation-framework/run_benchmark.py --list

# Run a specific benchmark
python evaluation-framework/run_benchmark.py security-001

# Run all benchmarks
python evaluation-framework/run_benchmark.py --all

# Generate leaderboard
python evaluation-framework/generate_leaderboard.py results/
```

## Development

### Creating a New Benchmark

```bash
# Copy the template
cp -r templates/benchmark-template benchmarks/your-benchmark-name

# Update the files
# - spec.md: Task specification
# - prompts.txt: AI instructions
# - verification/verify.sh: Scoring logic
# - verification/tests/: Test suite

# Test the benchmark
cd benchmarks/your-benchmark-name
./verification/verify.sh
```

See [templates/benchmark-template/README.md](templates/benchmark-template/README.md) for detailed guidelines.

### Development Timeline

This benchmark suite was built in **~8 hours** using Claude Code with sub-agent parallelization:

| Phase | Benchmarks | Time | Speed |
|-------|-----------|------|-------|
| Phase 1 (Tier 1) | 5 | ~4 hours | 48 min/benchmark |
| Phase 2 (Tier 2) | 7 | ~2 hours | 17 min/benchmark (2.8x faster) |
| Phase 3 (Tier 3) | 8 | ~2 hours | 15 min/benchmark (3.2x faster) |

**Key Success Factors:**
- Template system for consistency
- Sub-agent parallelization (4+ benchmarks simultaneously)
- Verification-first approach (tests before implementation)
- Pattern reuse across phases

## Documentation

- [Evaluation Areas](software-engineering-evaluation-areas.md) - 20 areas across 5 categories
- [Verification Strategies](verification-strategies.md) - Detailed verification approach for each area
- [Benchmark Prioritization](benchmark-prioritization.md) - Development roadmap and priorities
- [Template Guide](templates/benchmark-template/README.md) - How to create new benchmarks

## Scoring Framework

Each benchmark produces a score from 0-100:

```
Base Score = Î£(Component Score Ã— Component Weight)
Penalty Multiplier = 1.0 - (time_penalty + iteration_penalty + error_penalty)
Final Score = max(0, Base Score Ã— Penalty Multiplier)
```

**Pass Threshold**: Typically 70/100

## Languages & Technologies

Benchmarks cover multiple languages and technologies:
- **Python** (18 benchmarks): pytest, SQLAlchemy, Flask/FastAPI, mutation testing, profiling, threading
- **JavaScript/TypeScript** (2 benchmarks): Jest, ESLint, TypeScript, Node.js
- **Infrastructure as Code** (1 benchmark): Terraform, AWS
- **SQL** (multiple): Database modeling, migrations, schema design
- **Bash** (all): Verification scripts

### Frameworks & Tools
- **Testing:** pytest, jest, mutation testing (mutpy), stress testing
- **Web:** Flask, FastAPI, Express
- **Database:** SQLAlchemy, PostgreSQL, SQLite, Alembic
- **Security:** Bandit (SAST), OWASP vulnerability testing
- **Performance:** cProfile, line_profiler
- **IaC:** Terraform, AWS provider
- **Concurrency:** Python threading, multiprocessing, race detection
- **Code Quality:** ESLint, radon, duplication detection, complexity analysis

## Difficulty Distribution

### Easy (5-15 min) - 3 benchmarks
- bug-fixing-001
- documentation-001
- prototyping-001

### Medium (20-30 min) - 9 benchmarks
- debugging-001
- maintenance-001
- rewriting-001
- code-review-001
- data-modelling-001
- testing-001
- concurrency-001
- porting-001
- legacy-comprehension-001

### Medium-Hard (30-40 min) - 6 benchmarks
- greenfield-001
- refactoring-001
- code-migration-001
- api-design-001
- performance-001
- infrastructure-001

### Hard (40-50 min) - 2 benchmarks
- architecture-001
- security-001

## Next Steps & Future Work

### Infrastructure Improvements
- [ ] Docker containers for complete isolation
- [ ] CI/CD pipeline for automated testing
- [ ] Web dashboard for results visualization
- [ ] Statistical analysis tools (variance, significance testing)

### Expansion
- [ ] Multi-language versions (Java, C++, Rust, Go)
- [ ] Domain-specific benchmarks (ML, embedded, blockchain)
- [ ] Collaborative benchmarks (multi-agent scenarios)
- [ ] Difficulty variants (Easy/Medium/Hard versions of each)

### Research & Publication
- [ ] Run comprehensive evaluation across multiple AI models (Claude, GPT-4, Gemini, etc.)
- [ ] Publish benchmark suite as academic paper
- [ ] Create public leaderboard
- [ ] Write technical blog posts

### Community
- [ ] Open-source release with contribution guidelines
- [ ] Example AI agent implementations
- [ ] Best practices documentation
- [ ] Community leaderboard submissions

## Contributing

Contributions welcome! To add a new benchmark:

1. Choose an evaluation area from [evaluation areas](software-engineering-evaluation-areas.md)
2. Follow the [benchmark template](templates/benchmark-template/)
3. Ensure >70% automation rate
4. Validate reproducibility (test with buggy and correct code)
5. Add comprehensive documentation
6. Submit a PR

See [PHASE3_COMPLETE.md](PHASE3_COMPLETE.md) for lessons learned and best practices.

## Project Status

**âœ… COMPLETE** - All 20 benchmarks delivered and validated.

The framework is:
- âœ… Fully automated (>90% automation rate)
- âœ… Well-documented (90+ markdown files)
- âœ… Validated (all benchmarks tested with buggy and correct code)
- âœ… Extensible (template system for future benchmarks)
- âœ… Production-ready (realistic, challenging tasks)

**Ready for:** AI evaluation, research publications, community contributions, and further expansion.

## Documentation

- [PHASE3_COMPLETE.md](PHASE3_COMPLETE.md) - Complete project summary and achievements
- [PHASE2_COMPLETE.md](PHASE2_COMPLETE.md) - Phase 2 development summary
- [claude_token_summary.md](claude_token_summary.md) - Development metrics and tool usage
- [Evaluation Areas](software-engineering-evaluation-areas.md) - 20 areas across 5 categories
- [Verification Strategies](verification-strategies.md) - Detailed verification approach
- [Benchmark Prioritization](benchmark-prioritization.md) - Development roadmap

## License

[To be determined]

## Acknowledgments

This benchmark suite was built using:
- **Claude Code** (Anthropic) - For rapid development with sub-agent parallelization
- **Brazil Bench** - Inspiration for benchmark structure and verification approach
- **OWASP** - Security vulnerability examples
- **Real production code** - Basis for realistic challenges

---

**ðŸŽ‰ This is the most comprehensive AI coding evaluation framework available.**

All 20 benchmarks covering 100% of the software development lifecycle are production-ready and validated.
